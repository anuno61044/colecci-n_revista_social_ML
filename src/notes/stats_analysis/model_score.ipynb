{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/projects/colecci-n_revista_social_ML/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from common.social_model import SocialModel, BLIP_SocialModel, Random_SocialModel\n",
    "from typing import Callable\n",
    "from PIL import ImageFile, Image\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "import pandas as pd\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_results(\n",
    "        results_1 : list[str], \n",
    "        results_2 : list[str], \n",
    "        true_captions : list[str], \n",
    "        metric : Callable, \n",
    "        significance_level : float = 0.05\n",
    "    ) -> bool:\n",
    "    \"\"\"\n",
    "    Compares two models' results based on some already captioned_images and determines if model_1 is better than model_2 .\\n\n",
    "    Returns True if model_1 is significantly better; otherwise, False.\n",
    "    \"\"\"\n",
    "\n",
    "    scores_1 = []\n",
    "    scores_2 = []\n",
    "    for true_caption, generated_pair in zip(true_captions, zip(results_1, results_2)):\n",
    "        generated_caption1, generated_caption2 = generated_pair\n",
    "        scores_1.append(metric(true_caption, generated_caption1))\n",
    "        scores_2.append(metric(true_caption, generated_caption2))\n",
    "\n",
    "    _, p_value = ttest_rel(scores_1, scores_2)\n",
    "\n",
    "    NORMAL_CONST = 1.96 # 95% interval confidence\n",
    "    # NORMAL_CONST = 1.645 # 90% interval confidence\n",
    "    mean_diff = np.mean(np.array(scores_1) - np.array(scores_2))\n",
    "    std_diff = np.std(np.array(scores_1) - np.array(scores_2))\n",
    "\n",
    "    margin_of_error = NORMAL_CONST * std_diff / np.sqrt(len(scores_1))\n",
    "    conf_int = (mean_diff - margin_of_error, mean_diff + margin_of_error)\n",
    "\n",
    "    mean_1, std_1 = np.mean(scores_1), np.std(scores_1)\n",
    "    mean_2, std_2 = np.mean(scores_2), np.std(scores_2)\n",
    "    cohen_d = (mean_1 - mean_2) / np.sqrt((std_1**2 + std_2**2) / 2)\n",
    "\n",
    "\n",
    "    # Combine conditions\n",
    "    is_stat_significant = p_value < significance_level\n",
    "    is_confidence_valid = conf_int[0] > 0 or conf_int[1] < 0\n",
    "    is_effect_size_meaningful = cohen_d > 0.2  # Small effect threshold\n",
    "\n",
    "    print(f'p-value: {p_value}')\n",
    "    print(f'confidence-interval: {conf_int}')\n",
    "    print(f'effect-size: {cohen_d}')\n",
    "\n",
    "    return bool(is_stat_significant and is_confidence_valid and is_effect_size_meaningful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(\n",
    "    model_1 : SocialModel, \n",
    "    model_2: SocialModel, \n",
    "    captioned_images : list[tuple[str, str]], \n",
    "    metric : Callable, \n",
    "    significance_level : float = 0.05\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Compares two models based on some already captioned_images and determines if model_1 is better than model_2 .\\n\n",
    "    Returns True if model_1 is significantly better; otherwise, False.\n",
    "    \"\"\"\n",
    "\n",
    "    true_captions = [pair[1] for pair in captioned_images]\n",
    "    generated_captions_1 = []\n",
    "    generated_captions_2 = []\n",
    "    for image_path, _ in captioned_images:\n",
    "        generated_captions_1.append(model_1.caption(image_path))\n",
    "        generated_captions_2.append(model_2.caption(image_path))\n",
    "\n",
    "    return compare_models_results(\n",
    "        generated_captions_1, \n",
    "        generated_captions_2, \n",
    "        true_captions, \n",
    "        metric,\n",
    "        significance_level\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../../external/datasets/flickr8/'\n",
    "captions_csv = pd.read_csv(f'{dataset_path}captions.txt')\n",
    "THRESHOLD = 10\n",
    "\n",
    "captioned_set = set()\n",
    "captioned_images = []\n",
    "\n",
    "for value in captions_csv.values:\n",
    "    image_name = value[0]\n",
    "    image_caption = value[1] \n",
    "\n",
    "    if image_name not in captioned_set:\n",
    "        captioned_set.add(image_name)\n",
    "        image_path = f'{dataset_path}/Images/{image_name}'\n",
    "        captioned_images.append((image_path, image_caption))\n",
    "    if len(captioned_set) >= THRESHOLD:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meteor_metric(str1 : str, str2 : str) -> float:\n",
    "    tokenizer = SpaceTokenizer()\n",
    "    tokens1 = tokenizer.tokenize(str1)\n",
    "    tokens2 = tokenizer.tokenize(str2)\n",
    "    return single_meteor_score(tokens1, tokens2)\n",
    "\n",
    "def bleu_metric(str1 : str, str2 : str) -> float:\n",
    "    smooth_fn = SmoothingFunction().method4\n",
    "    tokenizer = SpaceTokenizer()\n",
    "    tokens1 = tokenizer.tokenize(str1)\n",
    "    tokens2 = tokenizer.tokenize(str2)\n",
    "    return sentence_bleu([tokens1], tokens2, smoothing_function=smooth_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.0008658795285143208\n",
      "confidence-interval: (np.float64(0.1380198434003062), np.float64(0.30768779973773297))\n",
      "effect-size: 2.271568666318965\n",
      "p-value: 0.0007518885897098205\n",
      "confidence-interval: (np.float64(-0.3151024444733975), np.float64(-0.14395279224752938))\n",
      "effect-size: -2.3510398703305166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLIP_PATH = '../../../external/trained_models/blip_image_captioning_base'\n",
    "model1 = BLIP_SocialModel(BLIP_PATH)\n",
    "model2 = Random_SocialModel()\n",
    "\n",
    "result1 = compare_models(model1, model2, captioned_images, meteor_metric)\n",
    "result2 = compare_models(model2, model1, captioned_images, meteor_metric)\n",
    "result1, result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: nan\n",
      "confidence-interval: (np.float64(0.0), np.float64(0.0))\n",
      "effect-size: 0.0\n",
      "p-value: nan\n",
      "confidence-interval: (np.float64(0.0), np.float64(0.0))\n",
      "effect-size: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23789/3145661190.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  cohen_d = (mean_1 - mean_2) / np.sqrt((std_1**2 + std_2**2) / 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = compare_models(model1, model1, captioned_images, meteor_metric)\n",
    "result2 = compare_models(model2, model2, captioned_images, meteor_metric)\n",
    "result1, result2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
