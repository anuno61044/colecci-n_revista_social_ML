{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.partition import get_random_partition\n",
    "import random\n",
    "import json\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        encoding = self.processor(images=item[\"image\"], text=random.choice(item[\"text\"]), padding=\"max_length\", return_tensors=\"pt\")\n",
    "        # remove batch dimension\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training function\n",
    "\n",
    "`train()` handles the training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs = 3):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        for _, batch in enumerate(dataloader):\n",
    "            input_ids = batch.pop(\"input_ids\").to(device)\n",
    "            pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                            pixel_values=pixel_values,\n",
    "                            labels=input_ids)\n",
    "\n",
    "            loss = outputs.loss\n",
    "\n",
    "            print(\"Loss:\", loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "- **sample_index**: index of the sample that's gonna be used for training\n",
    "- **subsample_ratio**: specifies the proportion between the partition samples and the sample of images used\n",
    "- **model_path**: path to pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 0\n",
    "subsample_ratio = 0.25\n",
    "model_path = '../../../trained_models/blip_image_captioning_base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load processor and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processor and model\n",
    "processor = BlipProcessor.from_pretrained(model_path)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sample and build subsamples from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[141, 141, 141, 143]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = '../../../datasets/flickr8/'\n",
    "captions_csv = pd.read_csv(f'{dataset_path}captions.txt')\n",
    "\n",
    "with open(f'{dataset_path}random_samples_7.json', 'rb') as file:\n",
    "    random_samples = json.load(file)\n",
    "    \n",
    "images = random_samples[sample_index]\n",
    "samples = get_random_partition(images, subsample_ratio)\n",
    "\n",
    "print(f'Samples: {len(samples)}')\n",
    "[len(samples[i]) for i, _ in enumerate(samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with each subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    print(f'Start on sample {i + 1} out of {len(samples)}')\n",
    "    data_dict = {}\n",
    "\n",
    "    for image in sample:\n",
    "        data_dict[image] = None\n",
    "\n",
    "    for value in captions_csv.values:\n",
    "        image_name = value[0]\n",
    "        image_caption = value[1]\n",
    "\n",
    "        if image_name in data_dict and data_dict[image_name] != None:\n",
    "            data_dict[image_name]['text'].append(image_caption)\n",
    "        elif image_name in data_dict:\n",
    "            image = Image.open(f'{dataset_path}/Images/{image_name}').convert('RGB')\n",
    "            data_dict[image_name] = {'image': image, 'text': [image_caption]}\n",
    "\n",
    "    data_list = list(data_dict.values())\n",
    "    print('Done processing images')\n",
    "\n",
    "    dataset = datasets.Dataset.from_list(data_list)\n",
    "    train_dataset = ImageCaptioningDataset(dataset, processor)\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1)\n",
    "    print('Done building the training dataset')\n",
    "\n",
    "    print('Start training')\n",
    "    train(model, train_dataloader)\n",
    "    print(f'Done training with sample {i + 1} out of {len(samples)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "date_id = date.isoformat(date.today())\n",
    "model.save_pretrained(f\"../../../trained_models/blip_image_captioning_{date_id}\")\n",
    "processor.save_pretrained(f\"../../../trained_models/blip_image_captioning_{date_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
