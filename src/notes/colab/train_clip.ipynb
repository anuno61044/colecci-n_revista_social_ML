{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1F9D82vMQJtrZA1I-M6Oba4tdjaKCLl9h","authorship_tag":"ABX9TyPP6zMlcVCG9f1S6ipQxZNO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install datasets\n","!pip install deep_translator"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zdE1woEeAcj2","executionInfo":{"status":"ok","timestamp":1738264714170,"user_tz":300,"elapsed":10128,"user":{"displayName":"Alvaro Gomzalez Brito","userId":"09958398185969828461"}},"outputId":"4333163f-6027-491a-def4-7ab149ec00bd"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: deep_translator in /usr/local/lib/python3.11/dist-packages (1.11.4)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.12.3)\n","Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2024.12.14)\n"]}]},{"cell_type":"code","source":["import json\n","from tqdm import tqdm\n","from PIL import Image\n","import pandas as pd\n","import torch\n","import random\n","import datasets\n","from torch.utils.data import Dataset, DataLoader\n","import json\n","import pandas as pd\n","from sqlalchemy import create_engine\n","from deep_translator import GoogleTranslator"],"metadata":{"id":"zTm0LlTwAab6","executionInfo":{"status":"ok","timestamp":1738264723929,"user_tz":300,"elapsed":9762,"user":{"displayName":"Alvaro Gomzalez Brito","userId":"09958398185969828461"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VZVj6sQs_pFD","executionInfo":{"status":"ok","timestamp":1738264740257,"user_tz":300,"elapsed":16331,"user":{"displayName":"Alvaro Gomzalez Brito","userId":"09958398185969828461"}},"outputId":"597387a5-6abb-45fc-ef31-14a605415b99"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["from transformers import CLIPProcessor, CLIPModel\n","\n","model_name = \"openai/clip-vit-large-patch14\"\n","\n","model = CLIPModel.from_pretrained(model_name)\n","processor = CLIPProcessor.from_pretrained(model_name)"]},{"cell_type":"code","source":["BASE_PATH = '/content/drive/MyDrive/social/'\n","\n","images_path = f'{BASE_PATH}images/'\n","db_path = f'{BASE_PATH}metadata.db'\n","train_json_path = f'{BASE_PATH}train_images.json'"],"metadata":{"id":"TjxkxVLaARzj","executionInfo":{"status":"ok","timestamp":1738264740258,"user_tz":300,"elapsed":7,"user":{"displayName":"Alvaro Gomzalez Brito","userId":"09958398185969828461"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Define the dataset class\n","class ImageCaptioningDataset(Dataset):\n","    def __init__(self, data, processor, max_length=77): # Add max_length\n","        \"\"\"\n","        Args:\n","            data: List of tuples (image_path, caption).\n","            processor: CLIPProcessor for image and text preprocessing.\n","            max_length: Maximum sequence length for text.\n","        \"\"\"\n","        self.data = data\n","        self.processor = processor\n","        self.max_length = max_length # Store max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        image= self.data[idx]['image']\n","        caption = self.data[idx]['text']\n","        inputs = self.processor(images=image, text=caption, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length) # Add truncation and max_length\n","\n","        return {\n","            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n","            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n","            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n","        }"],"metadata":{"id":"SWefuBcLATIU","executionInfo":{"status":"ok","timestamp":1738264740258,"user_tz":300,"elapsed":6,"user":{"displayName":"Alvaro Gomzalez Brito","userId":"09958398185969828461"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"W3FJmvUp32Bn","executionInfo":{"status":"ok","timestamp":1738264951444,"user_tz":300,"elapsed":211191,"user":{"displayName":"Alvaro Gomzalez Brito","userId":"09958398185969828461"}}},"outputs":[],"source":["engine = create_engine(f'sqlite:///{db_path}')\n","metadata_df = pd.read_sql('ImageData', engine)\n","\n","translator = GoogleTranslator(source='es', target='en')\n","data_dict = {}\n","train_images_dict : dict[str, str]\n","with open(train_json_path, \"r\") as file:\n","    train_images_dict = json.load(file)\n","\n","for id, name in train_images_dict.items():\n","    caption = metadata_df[metadata_df['id'] == int(id)][['caption']].values[0][0]\n","    image = Image.open(f'{images_path}{name}')\n","    caption = translator.translate(caption)\n","    data_dict[id] = {'image': image, 'text': [caption]}"]},{"cell_type":"code","source":["data_list = list(data_dict.values())\n","dataset = datasets.Dataset.from_list(data_list)"],"metadata":{"id":"sXbpLg3kCwTV","executionInfo":{"status":"ok","timestamp":1738264951444,"user_tz":300,"elapsed":8,"user":{"displayName":"Alvaro Gomzalez Brito","userId":"09958398185969828461"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"id":"6ivZRQIxBSZi","executionInfo":{"status":"ok","timestamp":1738264951445,"user_tz":300,"elapsed":8,"user":{"displayName":"Alvaro Gomzalez Brito","userId":"09958398185969828461"}}},"outputs":[],"source":["train_dataset = ImageCaptioningDataset(dataset, processor)\n","train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1)"]},{"cell_type":"code","source":["print(train_dataset[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jK6zOO5xaZ-f","executionInfo":{"status":"ok","timestamp":1738264951445,"user_tz":300,"elapsed":8,"user":{"displayName":"Alvaro Gomzalez Brito","userId":"09958398185969828461"}},"outputId":"a9e40636-582a-4eb0-a780-6408ba56305c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{'pixel_values': tensor([[[1.4486, 1.4194, 1.4340,  ..., 1.4194, 1.3464, 1.3902],\n","         [1.4632, 1.4486, 1.4340,  ..., 1.4048, 1.3902, 1.3902],\n","         [1.4632, 1.4340, 1.4340,  ..., 1.3902, 1.3902, 1.3756],\n","         ...,\n","         [1.5946, 1.5946, 1.5946,  ..., 1.5216, 1.4778, 1.5070],\n","         [1.5946, 1.6092, 1.5946,  ..., 1.5216, 1.4632, 1.4924],\n","         [1.5654, 1.5800, 1.5800,  ..., 1.5800, 1.5216, 1.5362]],\n","\n","        [[1.4446, 1.3995, 1.4446,  ..., 1.4295, 1.3545, 1.3995],\n","         [1.4446, 1.4145, 1.4446,  ..., 1.4145, 1.3995, 1.3995],\n","         [1.4446, 1.4145, 1.4446,  ..., 1.3995, 1.3995, 1.3845],\n","         ...,\n","         [1.6697, 1.6697, 1.6697,  ..., 1.6096, 1.5496, 1.5646],\n","         [1.6697, 1.6697, 1.6697,  ..., 1.5946, 1.5346, 1.5496],\n","         [1.6997, 1.6997, 1.7147,  ..., 1.6096, 1.5646, 1.5646]],\n","\n","        [[1.4491, 1.4065, 1.4491,  ..., 1.3780, 1.3069, 1.3496],\n","         [1.4633, 1.4349, 1.4349,  ..., 1.3638, 1.3496, 1.3496],\n","         [1.4633, 1.4349, 1.4349,  ..., 1.3496, 1.3496, 1.3354],\n","         ...,\n","         [1.7193, 1.7193, 1.7193,  ..., 1.6624, 1.6198, 1.6198],\n","         [1.7193, 1.7193, 1.7193,  ..., 1.6482, 1.5913, 1.5913],\n","         [1.7193, 1.7335, 1.7477,  ..., 1.6198, 1.5771, 1.5629]]]), 'input_ids': tensor([49406,   320,   269,   320,   320,  4255,   269, 14057,   657,  3581,\n","          537,   988,   269,    68,  1054,   756,   593,   988,   269, 11209,\n","         2982,   269, 49407]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"]}]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)\n","\n","# Define optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n","\n","# Training loop\n","num_epochs = 3  # Adjust epochs as needed\n","model.train()\n","\n","for epoch in range(num_epochs):\n","    loop = tqdm(train_dataloader, leave=True)\n","\n","    for batch in loop:\n","        optimizer.zero_grad()\n","\n","        # Move data to device\n","        pixel_values = batch[\"pixel_values\"].to(device)\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","\n","        # Forward pass\n","        outputs = model(pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask)\n","        image_embeds = outputs.image_embeds  # Image embeddings\n","        text_embeds = outputs.text_embeds  # Text embeddings\n","\n","        # Normalize embeddings\n","        image_embeds = F.normalize(image_embeds, p=2, dim=-1)\n","        text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n","\n","        # Compute cosine similarity\n","        logit_scale = model.logit_scale.exp()  # Scaling factor\n","        logits_per_image = torch.matmul(image_embeds, text_embeds.T) * logit_scale\n","        logits_per_text = logits_per_image.T  # Symmetric loss\n","\n","        # Generate labels (identity matrix)\n","        batch_size = pixel_values.shape[0]\n","        labels = torch.arange(batch_size, device=device)\n","\n","        # Compute contrastive loss\n","        loss_img = F.cross_entropy(logits_per_image, labels)\n","        loss_text = F.cross_entropy(logits_per_text, labels)\n","        loss = (loss_img + loss_text) / 2  # Final loss\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Update progress bar\n","        loop.set_description(f\"Epoch {epoch+1}\")\n","        loop.set_postfix(loss=loss.item())\n","\n","print(\"Training complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m3_QyioWEEah","executionInfo":{"status":"ok","timestamp":1738265257488,"user_tz":300,"elapsed":306048,"user":{"displayName":"Alvaro Gomzalez Brito","userId":"09958398185969828461"}},"outputId":"908e4343-a7e7-40f3-a8c1-3da61f9b11b0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 200/200 [01:34<00:00,  2.11it/s, loss=0]\n","Epoch 2: 100%|██████████| 200/200 [01:38<00:00,  2.03it/s, loss=0]\n","Epoch 3: 100%|██████████| 200/200 [01:42<00:00,  1.95it/s, loss=0]"]},{"output_type":"stream","name":"stdout","text":["Training complete!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"id":"UKyBLbYodIry","executionInfo":{"status":"ok","timestamp":1738265266668,"user_tz":300,"elapsed":9183,"user":{"displayName":"Alvaro Gomzalez Brito","userId":"09958398185969828461"}}},"outputs":[],"source":["# Save the model and processor locally\n","processor.save_pretrained(f\"{BASE_PATH}trained_models/{model_name}_model_tuned_en_cap\")\n","model.save_pretrained(f\"{BASE_PATH}trained_models/{model_name}_processor_tuned_en_cap\")"]}]}